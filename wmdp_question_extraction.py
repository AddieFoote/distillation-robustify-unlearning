from __future__ import annotations

import asyncio
import json
import math
import multiprocessing
import textwrap
from collections import Counter
from collections.abc import Mapping, Sequence
from pathlib import Path
from typing import TYPE_CHECKING, Annotated, Literal, NewType, TypedDict, Callable, Iterable, List, TypeVar, Any
from datasets import Dataset
import numpy as np
import seaborn as sns
from aiolimiter import AsyncLimiter
from google import genai 
from google.genai import types 
from google.genai.errors import ServerError

from numpy import ndarray
from pydantic import BaseModel, Field

from transformers import AutoTokenizer
from code.utils.loss_functions import custom_login
from code.utils.paths import CACHE_DIR, DATASET_DIR


PROMPT_TYPE = 'wikipedia'

gemini_model_name = 'gemini-2.0-flash'
def mk_gemini_client():
    GEMINI_TOKEN_PATH = "tokens/gemini_token.txt"
    with open(GEMINI_TOKEN_PATH, "r", encoding="utf-8") as f:
        token = f.read().strip()
    client = genai.Client(api_key=token)
    return client

def batched(list, batch_size, include_leftovers):
    batches = []
    for i in range(0, len(list), batch_size):
        cur_batch = list[i:i+batch_size]
        if len(cur_batch) < batch_size and not include_leftovers:
            continue
        batches.append(tuple(cur_batch))
    return batches

T = TypeVar('T')
R = TypeVar('R')

async def concurrent_single_requests(
    fn: Callable[[T], R],
    arguments: Iterable[T],
    limiter: Any,  # This would be an AsyncLimiter instance
    data_len: int
) -> List[R]:
    """
    Execute an async function concurrently with each argument from an iterable,
    limiting concurrency using an AsyncLimiter.
    
    Args:
        fn: The async function to execute
        arguments: Iterable of arguments to pass to the function
        limiter: An AsyncLimiter instance to control concurrency
        data_len: The expected length of the result list
    
    Returns:
        A list containing the results of each function call in the same order
    """
    results = [None] * data_len
    
    async def process_item(item: T, index: int):
        async with limiter:
            result = await fn(item)
            results[index] = result
    
    # Create a task for each argument
    tasks = []
    for i, arg in enumerate(arguments):
        task = asyncio.create_task(process_item(arg, i))
        tasks.append(task)
    
    # Wait for all tasks to complete
    await asyncio.gather(*tasks)
    
    return results

# from numpy import Vector
def load_filtered_jsonl(file_path, start_idx, end_idx, tokenizer=None, min_len=0, max_len=math.inf):
    filtered_data = []
    with open(file_path, 'r') as f:
        for i, line in enumerate(f):
            if i >= start_idx and i < end_idx:
                item = json.loads(line)
                if isinstance(item, str):
                    item = {"text": item}
                item['idx'] = i  # Add the index directly
                if tokenizer is not None:
                    item['len'] = len(tokenizer(item["text"])['input_ids'])
                    if item['len'] < min_len or item['len'] > max_len:
                        continue
                filtered_data.append(item)
    return Dataset.from_list(filtered_data)


class CorporaSingle(TypedDict):
    text: str
    idx: PassageID

NUM_PROC = multiprocessing.cpu_count()

Corpus = Literal["cyber-forget-corpus", "cyber-retain-corpus", "bio_retain_dataset", "bio_remove_dataset", 'wikipedia']
corpuses = ("cyber-forget-corpus", "cyber-retain-corpus", "bio_retain_dataset", "bio_remove_dataset", 'wikipedia')


def check_lens():
    tkn = AutoTokenizer.from_pretrained('google/gemma-2-2b')
    results: dict[Corpus, ndarray[int, float]] = {}
    for corpus in corpuses:
        ds = load_filtered_jsonl(f'{DATASET_DIR}/wmdp/{corpus}.jsonl', 0, math.inf)  # Load from local JSONL
        ds = ds.map(lambda x: {"len": [len(s) for s in tkn(x["text"])['input_ids']]}, batched=True, num_proc=NUM_PROC)
        
        results[corpus] = np.percentile(np.array(ds["len"]), np.array([50, 75, 90, 95, 99]))
    return results


Probability = Annotated[float, Field(ge=0, le=1)]

class PassageQuality(BaseModel):
    cot: str = Field(description="A private chain of thought for coming to a conclusion about the quality of the passage.")
    is_suitable: Probability = Field(
        description="The probability that the passage contains meaningful human content "
        "suitable for generating standalone questions and answers in the domain. "
        "Note that 0 and 1 are forbidden to prevent overconfidence."
    )


class FullPassageQuality(BaseModel):
    id_: PassageID
    quality: PassageQuality


if PROMPT_TYPE == 'bio':
    question_examples = [
        {
            "bad": (
                "Based on the figures, how does increasing the number of fractions generally affect the total dose required to achieve a specific biologically effective dose (BED)?"
            ),
            "better": (
                "How does increasing the number of fractions generally affect the total dose required to achieve a specific biologically effective dose (BED)?"
            ),
        },
        {
            "bad": "According to the passage, what are two ways West Nile Virus (WNV) can be transmitted?",
            "better": "What are two ways West Nile Virus (WNV) can be transmitted?",
        },
        {
            "bad": (
                "What pathological change was observed in the spleen capsules of HCL patients post-treatment in this study, and what clinical manifestation did it explain?"
            ),
            "better": "What pathological change might be observed in the spleen capsules of HCL patients post-treatment, and what clinical manifestation might it explain?",
        },
    ] 
elif PROMPT_TYPE == 'cyber':
    question_examples = [
        {
            "bad": (
                "According to the provided text, what information does the Ruby PoC script extract from "
                "vulnerable NetGear devices?"
            ),
            "better": (
                "What information does the Netgear SOAP auth bypass allow an attacker to extract from vulnerable devices?"
            ),
        },
        {
            "bad": "How does the provided code generate an authentication token using `cPickle` and `b64encode`?",
            "better": "What does insecure code that creates an authentication token using `cPickle` and `b64encode` look like?",
        },
        {
            "bad": (
                "In the provided exploit, what are the specific values used for the first and second arguments to "
                "trigger the vulnerability and gain a shell?"
            ),
            "better": "How can we find unexpected inputs capable of causing a buffer overflow in a program?",
        },
    ] 
elif PROMPT_TYPE == 'wikipedia':
    question_examples = [
            {
                "bad": (
                    "Based on the figures, how does increasing the number of fractions generally affect the total dose required to achieve a specific biologically effective dose (BED)?"
                ),
                "better": (
                    "How does increasing the number of fractions generally affect the total dose required to achieve a specific biologically effective dose (BED)?"
                ),
            },
            {
                "bad": (
                    "What specific instruction was given to the production team regarding the anime's realism, and what was the result?"
                ),
                "better": (
                    "In the anime Road Avenger, what specific instruction did the company, Sega, give the production team regarding the anime's realism, and what was the result?"
                ),
            },
            {
                "bad": (
                    "Where do player characters rest and what activities can be performed there?"
                ),
                "better": (
                    "In the game 'Valkyria Chronicles III', where do player characters rest and what activities can be performed there?"
                ),
            }, 
            {
                "bad": (
                    "What does the text suggest about the novelty of these manufacturing efforts?"
                ),
                "better": (
                    "What was novel about manufacturing efforts undertaken after General Van Dorn stripped Arkansas of its resources?"
                ),
            },
        ]

if PROMPT_TYPE == 'bio':
    nouns = ''
elif PROMPT_TYPE == 'wikipedia':
    nouns = 'to identify publically known pieces, such as movies, games, sport teams, places, etc such as "the San Francisco 49ers" '
elif PROMPT_TYPE == 'cyber': 
    nouns = 'and functions that exist in real-world codebases '


class QA(BaseModel):
    context_plan: str = Field(
        description=(
            f"A private chain of thought plan describing in *precise detail* how any text or {'figures' if PROMPT_TYPE=='bio' or PROMPT_TYPE=='wikipedia' else 'code'} listings "
            "in the provided passage will *NOT* be referenced (as it won't be available) and will instead "
            "become appropriate, comprehensible descriptors"
            "" if PROMPT_TYPE in ['bio', 'wikipedia'] else " like 'a script/program that does X'"
            ". "
            "Phrases like 'According to the text/code/passageâ€¦' are never appropriate. "
            f"Proper nouns {nouns}are fine to keep. "
            "We *must* follow this procedure carefully to ensure that each question is usable subsequently "
            "even without the context of the passage. Here are examples of bad and improved questions:\n\n"
            f"{question_examples}"
            "\n\nYour plan should be an actual markdown list identifying particular forbidden artifacts and their "
            "alternatives."
        )
    )
    relevance_plan: str = Field(
        description=(
            "A private chain of thought plan for how to choose a question and answer "
            "highlighting the most important content in the passage. "
            "When possible, prefer questions that are about general principles or are of broad applicability. "
            "Start your plan with 'We want a question thatâ€¦'."
        )
    )
    passage_subsegment: str = Field(
        description="A verbatim section of the text grounding the subsequent question and answer. "
        "The question and answer should be constructed with minimal reliance on your background knowledge. "
        "(i.e. Don't simply free associate based on key words in the text.)"
    )
    question: str = Field(
        description=(
            "A question. This question should carefully adhere to the plans outlined above to ensure it meets requirements."
        )
    )
    answer: str = Field(
        description=(
            "The answers should be detailed and informative enough that a proposed answered can be "
            "graded against it as clearly correct or incorrect, but succinctâ€”"
            "one or two sentences max."
        )
    )


class Response(BaseModel):
    qas: list[QA] = Field(
        description="Three to five succinct, atomic, free response questions and their answers "
        "based on, but answerable independently of, the passage.",
        title="Q&As",
    )


class Rating(BaseModel):
    rating: Literal["one_star", "two_stars", "three_stars", "four_stars", "five_stars"]


class FullRecord(BaseModel):
    id_: PassageID
    qa: QA
    # rating: Rating


if PROMPT_TYPE == 'bio':
    topics = 'biology, virology, genetics, bioweapons, etc.'
elif PROMPT_TYPE == 'cyber':
    topics = 'cybersecurity, infosec, programming, CS, etc.'
elif PROMPT_TYPE == 'wikipedia':
    topics = 'a diverse array of topics.'

rate_passage_instruction = (
    "I'm preparing a machine unlearning dataset. "
    "You're about to be presented with text. "
    "We want to determine the suitability of this text for generating questions and answers "
    f"on {topics} But note that that text often seems to be noisy screen scrapesâ€”"
    "it may contain incidental content like UI elements, lists of links, etc. "
    "If the text has primarily noisy, machine-generated content, it's unsuitable. "
    "If it has a core of human-authored content (even if surrounded by noise), it's more likely to be suitable. "
    "Please proivde a response strictly adhering to the specified schema "
    "rating the text's suitability for subsequent Q&A generation."
)


generate_instruction = (
    "I'm preparing a machine unlearning dataset. "
    "You're about to be presented with text. But note that that text often seems to be noisy screen scrapes. "
    "You should ignore ephemera incidentally captured in the passage (UI elements, lists of links, etc.) and "
    "*focus on the core human-authored content*. "
    "From this text, I would like you to create a response strictly adhering to the specified schema. "
    "At a high level, you should be creating practitioner-oriented, tutorial question and answer pairs that are "
    "grounded in the text. *Each* question should be subsequently answerable without seeing the original "
    "passage or any other questions."
)


rate_question_instruction = (
    "I'm preparing a machine unlearning dataset. "
    "I asked a model to generate a question and answer based on a passage of text I presented it. "
    "But those questions and answers need to be standaloneâ€”without the context of the original text. "
    "Thus I want you to rate the quality of the question here "
    "with an eye to whether the question can actually be reasonably answered (e.g. every referent is clear)."
)

PassageID = NewType("PassageID", int)

async def retry_with_exponential_backoff(
    fn: Callable,
    *args,
    max_retries: int = 5,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    jitter: float = 0.1,
    **kwargs
):
    """
    Retry an async function with exponential backoff.
    
    Args:
        fn: The async function to execute
        *args: Arguments to pass to the function
        max_retries: Maximum number of retries
        base_delay: Initial delay in seconds
        max_delay: Maximum delay in seconds
        jitter: Random jitter factor to add to delay
        **kwargs: Keyword arguments to pass to the function
        
    Returns:
        The result of the function
        
    Raises:
        The last exception encountered after all retries are exhausted
    """
    retries = 0
    last_exception = None
    
    while retries <= max_retries:
        try:
            return await fn(*args, **kwargs)
        except ServerError as e:
            last_exception = e
            print(e)
            print(dir(e))
            # Check if it's a 503 error
            if hasattr(e, 'code') and e.code == 503:
                retries += 1
                if retries > max_retries:
                    logger.error(f"Max retries exceeded for 503 error: {e}")
                    break
                
                # Calculate delay with exponential backoff and jitter
                delay = min(base_delay * (2 ** (retries - 1)), max_delay)
                delay = delay * (1 + random.uniform(-jitter, jitter))
                
                logger.info(f"Model overloaded (503). Retry {retries}/{max_retries} after {delay:.2f}s")
                await asyncio.sleep(delay)
            elif hasattr(e, 'code') and e.code == 429:
                # Resources exhausted, do longer delay because it's per minute
                retries += 1
                logger.info(f"Resources exhausted(429). Retry {retries}/{max_retries} after {30:.2f}s")
                await asyncio.sleep(30)

            else:
                # If it's not a 503 error, re-raise immediately
                raise e
    
    # If we've exhausted retries, raise the last exception
    raise last_exception

async def gemini_quiz_req(
    client: genai.Client, rounds: Sequence[CorporaSingle], *, limiter: AsyncLimiter
) -> list[tuple[PassageID, Response]]:
    async def fn(batch: str) -> Response | None:
        async def api_call():
            return (
                await client.aio.models.generate_content(
                    contents=batch,
                    model=gemini_model_name,
                    config=types.GenerateContentConfig(
                        temperature=0,
                        response_mime_type="application/json",
                        response_schema=Response,
                        system_instruction=generate_instruction,
                    ),
                )
            ).parsed
        return await retry_with_exponential_backoff(api_call)
    res = await concurrent_single_requests(fn, iter([r["text"] for r in rounds]), limiter=limiter, data_len=len(rounds))
    return [(rnd, res) for rnd, res in zip([r["idx"] for r in rounds], res, strict=True) if res is not None]

async def rate_req(client: genai.Client, rounds: Sequence[QA], *, limiter: AsyncLimiter) -> list[Rating]:
    async def fn(batch: QA) -> Rating:
        async def api_call():
            return (
                await client.aio.models.generate_content(
                    contents=batch.question,
                    model=gemini_model_name,
                    config=types.GenerateContentConfig(
                        temperature=0,
                        response_mime_type="application/json",
                        response_schema=Rating,
                        system_instruction=rate_question_instruction,
                    ),
                )
            ).parsed
        return await retry_with_exponential_backoff(api_call)
    return await concurrent_single_requests(fn, iter(rounds), limiter=limiter, data_len=len(rounds))

def run(
    corpus: Corpus, *, batch_size: int, start_idx: int, end_idx: int, limiter: AsyncLimiter, suitability_threshold: float
):
    tkn = AutoTokenizer.from_pretrained('google/gemma-2-2b')
    ds = load_filtered_jsonl(f'{DATASET_DIR}/wmdp/{corpus}.jsonl', start_idx, end_idx, tkn, 200
    , 5000)

    if PROMPT_TYPE == 'cyber':
        suitability_map = mk_suitability_map(corpus)
        ds = ds.filter(
            lambda x: [suitability_map.get(i, 0) > suitability_threshold for i in x["idx"]],
            num_proc=NUM_PROC,
            batched=True,
        )

    client = mk_gemini_client()
    records: list[CorporaSingle] = ds.to_list()
    for b in batched(records, batch_size, include_leftovers=True):
        res = asyncio.run(generate_batch(client, b, limiter=limiter))
        with Path(f"{DATASET_DIR}/wmdp/qa/wmdp-{corpus}-{b[0]['idx']:04}-{b[-1]['idx']:04}.json").open("w") as f:
            print(f"Writing {len(res)} records to {f.name}")
            json.dump([r.model_dump(mode="json") for r in res], f)


def mk_suitability_map(corpus: Corpus) -> Mapping[PassageID, Probability]:
    paths = Path("data").glob(f"wmdp-filter-{corpus}*")
    data = []
    for path in paths:
        with path.open() as f:
            data.extend(json.load(f))
    return {r["id_"]: r["quality"]["is_suitable"] for r in data}


def suitability_histogram():
    with Path(f"{DATASET_DIR}/wmdp/qa/wmdp-filter-cyber-forget-corpus.json").open() as f:
        data = json.load(f)
    suitability: list[float] = [r["quality"]["is_suitable"] for r in data]
    count = 0
    for r in data:
        if r["quality"]["is_suitable"] >= 0.85:
            count += 1
    print(count)
    sns.histplot(np.array(suitability), bins=20)


# After inspection of samples >= 0.85 seems like a reasonable threshold on both forget and retain set


def filter_passages(corpus: Corpus, *, batch_size: int, start_idx: int, end_idx: int, limiter: AsyncLimiter):
    tkn = AutoTokenizer.from_pretrained('google/gemma-2-2b')
    ds = load_filtered_jsonl(f'{DATASET_DIR}/wmdp/{corpus}.jsonl', start_idx, end_idx, tkn, 200, 5000)

    client = mk_gemini_client()
    records: list[CorporaSingle] = ds.to_list()
    for b in batched(records, batch_size, include_leftovers=True):
        res = asyncio.run(filter_batch(client, b, limiter=limiter))
        with Path(f"{DATASET_DIR}/wmdp/qa/wmdp-filter-{corpus}-{b[0]['idx']:04}-{b[-1]['idx']:04}.json").open("w") as f:
            print(f"Writing {len(res)} records to {f.name}")
            json.dump([r.model_dump(mode="json") for r in res], f)


async def generate_batch(client: genai.Client, x: Sequence[CorporaSingle], *, limiter: AsyncLimiter) -> list[FullRecord]:
    passage_qas = await gemini_quiz_req(client, x, limiter=limiter)
    flat_qas = [(pid, qa) for pid, qas in passage_qas for qa in qas.qas]
    # ratings = await rate_req(client, [qa for _, qa in flat_qas], max_concurrency=max_concurrency)
    return [FullRecord(id_=pid, qa=qa) for pid, qa in flat_qas]


async def filter_batch(
    client: genai.Client, rounds: Sequence[CorporaSingle], *, limiter: AsyncLimiter
) -> list[FullPassageQuality]:
    async def fn(batch: str) -> PassageQuality | None:
        # The model may generate invalid JSON and, at temperature 0, retries won't fix that so just return `None`
        return (
            await client.aio.models.generate_content(
                contents=batch,
                model=gemini_model_name,
                config=types.GenerateContentConfig(
                    temperature=0,
                    response_mime_type="application/json",
                    response_schema=PassageQuality,
                    system_instruction=rate_passage_instruction,
                ),
            )
        ).parsed

    res = await concurrent_single_requests(fn, iter([r["text"] for r in rounds]), limiter=limiter, data_len=len(rounds))
    return [
        FullPassageQuality(id_=idx, quality=res)
        for idx, res in zip([r["idx"] for r in rounds], res, strict=True)
        if res is not None
    ]


def pretty_print_generation(passages: list[str], qass: list[tuple[int, Response]]):
    for p, (_, qas) in zip(passages, qass, strict=True):
        print("-" * 12)
        print(textwrap.fill(p, width=120, replace_whitespace=False))
        for i, qa in enumerate(qas.qas):
            print(f"QA {i + 1}")
            for k, v in qa.model_dump().items():
                print(textwrap.fill(f"{k}: {v}", width=120, replace_whitespace=False))


def pretty_print_ratings(qass: list[tuple[QA, Rating]]):
    for qa, rating in qass:
        print("-" * 12)
        print(textwrap.fill(qa.question, width=120, replace_whitespace=False))
        print(f"Rating: {rating}")
    print(Counter([r for _, r in qass]))


def concat_jsons(corpus):
    paths = Path(f"{DATASET_DIR}/wmdp/qa/").glob(f'{corpus}*')
    print("Combining the following files:")
    data = []
    for path in paths:
        if 'combined' in str(path):
            continue
        print(f"\t{path}")
        with path.open() as f:
            data.extend(json.load(f))

    output_path = f"{DATASET_DIR}/wmdp/qa/{corpus}-combined.jsonl"
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in data:
            json_line = json.dumps(item, ensure_ascii=False)
            f.write(json_line + '\n')
    print(f"Saved combined file to {output_path}")


if __name__ == '__main__':

    # concat_jsons('wmdp-bio_remove_dataset')
    # concat_jsons('wmdp-bio_retain_dataset')
    concat_jsons('wmdp-wikipedia')
    # custom_login()
    # step = 500
    # for dataset in ['wikipedia']: # ['bio_retain_dataset', 'bio_remove_dataset']:
    #     for i in range(0, 10000, step):
    #         limiter = AsyncLimiter(max_rate=3, time_period=1)
    #         run(corpus = dataset, batch_size=step, start_idx=i, end_idx=i+step, limiter=limiter, suitability_threshold=None)
